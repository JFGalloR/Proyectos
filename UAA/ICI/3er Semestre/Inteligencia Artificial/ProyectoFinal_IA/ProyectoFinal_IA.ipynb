{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77290eb4",
   "metadata": {},
   "source": [
    "># Universidad Autónoma de Aguascalientes\n",
    ">## *Ingeniería en Computación Inteligente*\n",
    ">#### Materia:\n",
    ">Inteligencia Artificial\n",
    ">#### Topico: \n",
    ">Proyecto Final:\n",
    ">- _Reconocimiento de Expresiones con red neuronal._\n",
    ">#### Integrantes del Equipo:\n",
    ">- Juan Francisco Gallo Ramírez\n",
    ">- José Alfredo Díaz Robledo\n",
    ">- Luis Palbo Esparza Terrones\n",
    ">- Luis Manuel Flores Jiménez \n",
    ">#### Maestro: \n",
    ">Dr. Francisco Javier Luna Rosas\n",
    ">#### Fecha: \n",
    ">Noviembre del 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4462b08",
   "metadata": {},
   "source": [
    "# Proyecto Final: _Reconocimiento de Expresiones con Red Neuronal._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24e14b4",
   "metadata": {},
   "source": [
    "## ▪️ Se importan la librerías correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14c9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dded0fd",
   "metadata": {},
   "source": [
    "## ▪️ Se define la funcción para calcular distancias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c462b9",
   "metadata": {},
   "source": [
    "Se define la función que no ayuda a calcular la distancia entre los puntos faciales, esto para realizar el dataset que no ayudará a detectar las expresiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4270ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_distancia(punto1, punto2):\n",
    "    x1, y1, z1 = punto1.x, punto1.y, punto1.z\n",
    "    x2, y2, z2 = punto2.x, punto2.y, punto2.z\n",
    "    distancia = math.sqrt((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)\n",
    "    return distancia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b7b695",
   "metadata": {},
   "source": [
    "## ▪️ Se crea el archivo excel y se nombran las celdas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4fc3cc",
   "metadata": {},
   "source": [
    "El archivo excel es el que almacenará nuestro dataset de las distanticias entre los puntos faciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9239a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.active\n",
    "sheet['A1'] = \"Distancia entre Ceja y Ojo\";\n",
    "sheet['B1'] = \"Apertura de Ojo\";\n",
    "sheet['C1'] = \"Ancho de Boca\";  \n",
    "sheet['D1'] = \"Apertura de Boca\";\n",
    "sheet['E1'] = \"Distancia de Extremos y Superior Boca\";\n",
    "sheet['F1'] = \"Distancia de Extremos e Inferior Boca\";\n",
    "sheet['G1'] = \"EXPRESIÓN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c8458",
   "metadata": {},
   "source": [
    "## ▪️ Se define la función para obtener los datos faciales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1913cb",
   "metadata": {},
   "source": [
    "Esta función recibe como parametros la expresión, que es el clasificador que se colocará en la última celda; un booleano para establecer la preferencia de mostrar la máscara de puntos o no; la cantidad, que es la cantidad de fotogramas que se procesarán para el dataset; y finalmente el archivo excel donde se guardará el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52463bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillDataSet(expresion, show_mask, cantidad, excel):\n",
    "    i = 0\n",
    "    showMask = show_mask\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    # Inicializar la cámara.\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error al abrir la cámara.\")\n",
    "        exit()\n",
    "        \n",
    "    # Crear un objeto CascadeClassifier para la detección de rostros.\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Verificar si la cámara se abrió correctamente.\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                               max_num_faces=1,\n",
    "                               min_detection_confidence=0.5) as face_mesh:\n",
    "        \n",
    "        # Se capturarán los datos según la cantidad indicada.\n",
    "        while i < cantidad:\n",
    "            # Capturar un solo fotograma.\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Verificar si la lectura del fotograma fue exitosa\n",
    "            if not ret:\n",
    "                print(\"Error al leer el fotograma.\")\n",
    "                break\n",
    "\n",
    "            # Transformar el frame en escala de grises para una mejor detección.\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            # Realizar la detección de rostros.\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "            \n",
    "            for (x, y, w, h) in faces:\n",
    "                \n",
    "                # Dibujar un rectángulo alrededor del rostro.\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 220), 2)\n",
    "                # Dibujar texto por encima del cuadro.\n",
    "                cv2.putText(frame, \"Entrenando expresion:\" , (x, y-30), cv2.FONT_HERSHEY_PLAIN , 0.8, (0, 255, 220), 2)\n",
    "                cv2.putText(frame, expresion , (x, y-10), cv2.FONT_HERSHEY_SIMPLEX , 0.8, (255, 255, 255), 2)\n",
    "\n",
    "                    \n",
    "                # Recortar y guardar el rectángulo del rostro\n",
    "                face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # Se aplica la mascara de punto facemesh.\n",
    "                frame_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "                results = face_mesh.process(frame_rgb)\n",
    "\n",
    "                if results.multi_face_landmarks is not None:\n",
    "                    for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "                        #>>> PUNTOS DE BOCA >>>>>>>>>\n",
    "                        # Punto medio labio superior.\n",
    "                        p13 = face_landmarks.landmark[13]\n",
    "                        # Punto medio labio inferior.\n",
    "                        p14 = face_landmarks.landmark[14]\n",
    "                        # Punto extremo izquierdo.\n",
    "                        p61 = face_landmarks.landmark[61]\n",
    "                        # Punto extremo derecho.\n",
    "                        p306 = face_landmarks.landmark[306]\n",
    "\n",
    "                        #>>> PUNTOS DE CEJA >>>>>>>>>\n",
    "                        # Ceja izquierda.\n",
    "                        p65 = face_landmarks.landmark[65]\n",
    "                        # Ceja derecha.\n",
    "                        p295 = face_landmarks.landmark[295]\n",
    "\n",
    "                        #>>> PUNTOS DE OJO >>>>>>>>>\n",
    "                        # Ojo izquierdo ----\n",
    "                        # Punto superior.\n",
    "                        p159 = face_landmarks.landmark[159]\n",
    "                        # Punto inferior.\n",
    "                        p145 = face_landmarks.landmark[145]\n",
    "                        # Ojo derecho ------\n",
    "                        # Punto superior.\n",
    "                        p386 = face_landmarks.landmark[386]\n",
    "                        # Punto inferior.\n",
    "                        p374 = face_landmarks.landmark[374]\n",
    "                        sheet = wb.active\n",
    "\n",
    "                        # === Se hacen los respectivos cálculos de distancias entre puntos.\n",
    "                        # Distancia entre Ceja y Ojo.\n",
    "                        d1 = (calcular_distancia(p65, p159) + calcular_distancia(p295, p386)) / 2\n",
    "                        # Apertura de Ojo\".\n",
    "                        d2 = (calcular_distancia(p159, p145) + calcular_distancia(p386, p374)) / 2\n",
    "                        # Ancho de Boca.\n",
    "                        d3 = calcular_distancia(p61, p306)\n",
    "                        # Apertura de Boca.\n",
    "                        d4 = calcular_distancia(p13, p14)\n",
    "                        # Distancia de Extremos y Superior Boca.\n",
    "                        d5 = (calcular_distancia(p61, p13) + calcular_distancia(p306, p13)) / 2\n",
    "                        # Distancia de Extremos e Inferior Boca.\n",
    "                        d6 = (calcular_distancia(p61, p14) + calcular_distancia(p306, p14)) / 2\n",
    "                        # Clasificador.\n",
    "                        d7 = expresion\n",
    "\n",
    "                        if showMask:\n",
    "                        # Dibujar la malla facial en el fotograma recortado\n",
    "                            mp_drawing.draw_landmarks(\n",
    "                                face_roi, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "                                connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1, circle_radius=1)\n",
    "                            )\n",
    "                            \n",
    "                        i = i + 1\n",
    "                        # Se agregan los datos al dataset.\n",
    "                        sheet.append([d1, d2, d3, d4, d5, d6, d7])\n",
    "\n",
    "            # Mostrar el fotograma con los rostros detectados.\n",
    "            cv2.imshow('Sistema de Reconocimiento de Exrpesiones', frame)\n",
    "\n",
    "            # Salir del bucle si se presiona la tecla 'q'\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    # Liberar la cámara y cerrar la ventana.\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # Guardar en el archivo.\n",
    "    excel.save('DataSet.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86078415",
   "metadata": {},
   "source": [
    "### Se llena el dataset con expresión: _Neutral_ 😐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5304718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillDataSet(\"Neutral\", True, 700, wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050679d0",
   "metadata": {},
   "source": [
    "### Se llena el dataset con expresión: _Feliz_ 😁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30490e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillDataSet(\"Feliz\", True, 700, wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda7457",
   "metadata": {},
   "source": [
    "### Se llena el dataset con expresión: _Triste_ 🙁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af449802",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillDataSet(\"Triste\", True, 700, wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a028d1",
   "metadata": {},
   "source": [
    "### Se llena el dataset con expresión: _Enojado_ 😠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1097d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillDataSet(\"Enojado\", True, 700, wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf424d",
   "metadata": {},
   "source": [
    "### Se llena el dataset con expresión: _Sorprendido_ 😮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b9f887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillDataSet(\"Sorprendido\", True, 700, wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e281c",
   "metadata": {},
   "source": [
    "## ▪️ Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a976a",
   "metadata": {},
   "source": [
    "Se lee el archivo del dataset, y se muestran la respectiuva matriz categórica y matriz a predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1981a245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Se muestra la matriz categorica:\n",
      "\n",
      "[[0.10600105 0.05144231 0.26750478 0.01123014 0.15149762 0.14904705]\n",
      " [0.10756348 0.05149    0.28032289 0.01356024 0.15962655 0.15604199]\n",
      " [0.10639915 0.0484678  0.27683848 0.01160979 0.15743412 0.15481244]\n",
      " ...\n",
      " [0.12328187 0.05783282 0.29886252 0.12753206 0.18379155 0.17349823]\n",
      " [0.12418599 0.05882374 0.29708943 0.13052481 0.18329715 0.17251083]\n",
      " [0.12737746 0.05845203 0.29758004 0.10806184 0.17942204 0.16710349]]\n",
      "\n",
      ">> Se muestra la matriz a predecir:\n",
      "\n",
      "['Neutral' 'Neutral' 'Neutral' ... 'Sorprendido' 'Sorprendido'\n",
      " 'Sorprendido']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('DataSet.xlsx')\n",
    "X = data.iloc[0:,0:6]\n",
    "y = data.iloc[:,6]\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print( \">> Se muestra la matriz categorica:\\n\")\n",
    "print(X)\n",
    "print( \"\\n>> Se muestra la matriz a predecir:\\n\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36fe17f",
   "metadata": {},
   "source": [
    "Se separan los datos con el 70% de los datos para entrenamiento y el 30% para testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70bb6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a521c3",
   "metadata": {},
   "source": [
    "Mediante el constructor inicializa la instancia_red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f12a7e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(max_iter=2000, random_state=0, solver='lbfgs')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=2000, random_state=0, solver=&#x27;lbfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=2000, random_state=0, solver=&#x27;lbfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(max_iter=2000, random_state=0, solver='lbfgs')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instancia_red = MLPClassifier(solver='lbfgs', random_state=0, max_iter=2000)\n",
    "print(instancia_red)\n",
    "instancia_red.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d55739",
   "metadata": {},
   "source": [
    "Las predicciones del testing se muestran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f1ffe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Las predicciones en Testing son: ['Feliz' 'Neutral' 'Sorprendido' ... 'Sorprendido' 'Feliz' 'Neutral']\n"
     ]
    }
   ],
   "source": [
    "print(\">> Las predicciones en Testing son: {}\".format(instancia_red.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6de5bf",
   "metadata": {},
   "source": [
    "Función para calcular los índices de calidad de la predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b891538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_general(MC, nombres = None):\n",
    "    precision_global = np.sum(MC.diagonal()) / np.sum(MC)\n",
    "    error_global = 1 - precision_global\n",
    "    return {\">> Matriz de Confusión\":MC, \n",
    "            \">> Precisión Global\":precision_global, \n",
    "            \">> Error Global\":error_global}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd97a0",
   "metadata": {},
   "source": [
    "## ▪️ Índices de calidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8893178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Matriz de Confusión:\n",
      "[[206   0   0   0   0]\n",
      " [  0 210   0   0   0]\n",
      " [  0   0 223   0   0]\n",
      " [  0   0   0 216   0]\n",
      " [  0   0   0   0 195]]\n",
      "\n",
      ">> Precisión Global:\n",
      "1.0\n",
      "\n",
      ">> Error Global:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "prediccion = instancia_red.predict(X_test)\n",
    "MC = confusion_matrix(y_test, prediccion)\n",
    "indices = indices_general(MC,list(np.unique(y)))\n",
    "for k in indices:\n",
    "    print(\"\\n%s:\\n%s\"%(k,str(indices[k])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf8b57",
   "metadata": {},
   "source": [
    "Diccionario para texto y color según la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bab723ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "expresion = {\n",
    "    \"['Neutral']\" : [\"Neutral\", (123, 125, 125)],\n",
    "    \"['Feliz']\" : [\"Feliz\", (15, 196, 241)],\n",
    "    \"['Triste']\" : [\"Triste\", (193, 134, 46)],\n",
    "    \"['Enojado']\" : [\"Enojado\", (43, 57, 192)],\n",
    "    \"['Sorprendido']\" : [\"Sorprendido\", (99, 180, 40)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218cd85",
   "metadata": {},
   "source": [
    "## ▪️ Prueba de la red neuronal entrenada para reconocer expresiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8866fcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Ejecución exitosa.\n"
     ]
    }
   ],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Inicializar la cámara.\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error al abrir la cámara.\")\n",
    "    exit()\n",
    "    \n",
    "# Crear un objeto CascadeClassifier para la detección de rostros.\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Verificar si la cámara se abrió correctamente\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=False,\n",
    "                           max_num_faces=1,\n",
    "                           min_detection_confidence=0.5) as face_mesh:\n",
    "    while True:\n",
    "        # Capturar un solo fotograma\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Verificar si la lectura del fotograma fue exitosa\n",
    "        if not ret:\n",
    "            print(\"Error al leer el fotograma.\")\n",
    "            break\n",
    "\n",
    "        # Capturar un solo fotograma.\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Verificar si la lectura del fotograma fue exitosa\n",
    "        if not ret:\n",
    "            print(\"Error al leer el fotograma.\")\n",
    "            break\n",
    "\n",
    "        # Transformar el frame en escala de grises para una mejor detección.\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        # Realizar la detección de rostros.\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "\n",
    "            # Recortar y guardar el rectángulo del rostro\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Se aplica la mascara de punto facemesh.\n",
    "            frame_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(frame_rgb)\n",
    "\n",
    "            if results.multi_face_landmarks is not None:\n",
    "                for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "                    #>>> PUNTOS DE BOCA >>>>>>>>>\n",
    "                    # Punto medio labio superior.\n",
    "                    p13 = face_landmarks.landmark[13]\n",
    "                    # Punto medio labio inferior.\n",
    "                    p14 = face_landmarks.landmark[14]\n",
    "                    # Punto extremo izquierdo.\n",
    "                    p61 = face_landmarks.landmark[61]\n",
    "                    # Punto extremo derecho.\n",
    "                    p306 = face_landmarks.landmark[306]\n",
    "\n",
    "                    #>>> PUNTOS DE CEJA >>>>>>>>>\n",
    "                    # Ceja izquierda.\n",
    "                    p65 = face_landmarks.landmark[65]\n",
    "                    # Ceja derecha.\n",
    "                    p295 = face_landmarks.landmark[295]\n",
    "\n",
    "                    #>>> PUNTOS DE OJO >>>>>>>>>\n",
    "                    # Ojo izquierdo ----\n",
    "                    # Punto superior.\n",
    "                    p159 = face_landmarks.landmark[159]\n",
    "                    # Punto inferior.\n",
    "                    p145 = face_landmarks.landmark[145]\n",
    "                    # Ojo derecho ------\n",
    "                    # Punto superior.\n",
    "                    p386 = face_landmarks.landmark[386]\n",
    "                    # Punto inferior.\n",
    "                    p374 = face_landmarks.landmark[374]\n",
    "\n",
    "                    # === Se hacen los respectivos cálculos de distancias entre puntos.\n",
    "                    # Distancia entre Ceja y Ojo.\n",
    "                    d1 = (calcular_distancia(p65, p159) + calcular_distancia(p295, p386)) / 2\n",
    "                    # Apertura de Ojo\".\n",
    "                    d2 = (calcular_distancia(p159, p145) + calcular_distancia(p386, p374)) / 2\n",
    "                    # Ancho de Boca.\n",
    "                    d3 = calcular_distancia(p61, p306)\n",
    "                    # Apertura de Boca.\n",
    "                    d4 = calcular_distancia(p13, p14)\n",
    "                    # Distancia de Extremos y Superior Boca.\n",
    "                    d5 = (calcular_distancia(p61, p13) + calcular_distancia(p306, p13)) / 2\n",
    "                    # Distancia de Extremos e Inferior Boca.\n",
    "                    d6 = (calcular_distancia(p61, p14) + calcular_distancia(p306, p14)) / 2\n",
    "                    # Clasificador.\n",
    "                    d7 = expresion\n",
    "\n",
    "                    # Se reaiza la predicción.\n",
    "                    prediccion = instancia_red.predict(np.array([d1, d2, d3, d4, d5, d6]).reshape(1, -1))\n",
    "                    pred = str(prediccion)\n",
    "                    \n",
    "                    # Se escribe la predicción en el fotograma.\n",
    "                    cv2.putText(frame, \"Expresion:\" , (x, y-40), cv2.FONT_HERSHEY_PLAIN , 0.8, (255, 255, 255), 1)\n",
    "                    cv2.putText(frame, expresion[pred][0], (x, y-10), cv2.FONT_HERSHEY_DUPLEX , 1, expresion[pred][1], 2)\n",
    "\n",
    "        # Mostrar el fotograma con los rostros detectados\n",
    "        cv2.imshow('Sistema de Reconocimiento de Exrpesiones', frame)\n",
    "\n",
    "        # Salir del bucle si se presiona la tecla 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Liberar la cámara y cerrar la ventana\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\">>> Ejecución exitosa.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c96727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
